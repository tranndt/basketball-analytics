{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyYAML in /Users/jasetran/Jase/UM/Git/basketball-analytics/.venv/lib/python3.9/site-packages (6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display_html\n",
    "import datetime as dt\n",
    "from base import * \n",
    "import run_experiment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = get_datasets(['prior_scheduling',\n",
    "#     'prior_last10_stats_avg',\n",
    "#     'prior_cumu_stats_avg'], combine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name,dataframe = next(datasets)\n",
    "# print(dataset_name)\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "def rfecv_experiment(name=None, save_to='.', datasets=None, combine=False, label=None, models=None, scorer=None, save_best_model=False, cv=3, min_features_thresholds=[0.2,0.4,0.6,0.8,1.0]):\n",
    "    # Set default name if not provided\n",
    "    if name is None:\n",
    "        name = generate_name()\n",
    "    print(name)\n",
    "    # Create experiment directory\n",
    "    experiment_dir = f\"{save_to}/{name}\"\n",
    "    experiment_dir_results = f\"{experiment_dir}/results\"\n",
    "    experiment_dir_best_models = f\"{experiment_dir}/best_models\"\n",
    "    Path(experiment_dir).mkdir(parents=True, exist_ok=True)\n",
    "    Path(experiment_dir_results).mkdir(parents=True, exist_ok=True)\n",
    "    Path(experiment_dir_best_models).mkdir(parents=True, exist_ok=True)\n",
    "    save_experiment(\n",
    "        dict(name=name, save_to=experiment_dir, datasets=datasets, combine=combine, label=label, \n",
    "                models=models, scorer=scorer, save_best_model=save_best_model, cv=cv, min_features_thresholds=min_features_thresholds),\n",
    "        filepath= f'{experiment_dir}/experiment.yaml'\n",
    "    )\n",
    "    dataset_iterator = tqdm(get_datasets(datasets, combine))     # Get dataset iterator\n",
    "    y = get_dataset(label).values.flatten()     # Get label array\n",
    "    model_list = get_models(models)     # Get list of models\n",
    "    \n",
    "    for i,(dataset_name, dataset) in enumerate(dataset_iterator):\n",
    "        # Split data into train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.3, random_state=42)\n",
    "        # Loop over models\n",
    "        for j,model in enumerate(model_list):\n",
    "            # Loop over min features thresholds\n",
    "            features_thresholds = np.unique(np.ceil(np.array(min_features_thresholds) * len(dataset.columns))).astype(int) \n",
    "            for features_threshold in features_thresholds:\n",
    "                dataset_iterator.set_description(\n",
    "                    f'{dataset_name=} ({i+1}/{len(datasets)}), model={model.__class__.__name__} ({j+1}/{len(model_list)}), num_features={features_threshold}/{len(dataset.columns)}'\n",
    "                )\n",
    "                # Create RFECV object\n",
    "                rfecv = RFECV(estimator=model, cv=cv, min_features_to_select=int(features_threshold*len(dataset.columns)))\n",
    "                # Fit RFECV to training data\n",
    "                rfecv.fit(X_train, y_train)\n",
    "                reduced_features = list(dataset.columns[rfecv.support_])                 # Get list of reduced features\n",
    "                X_train_trf = rfecv.transform(X_train)                 # Get transformed training and testing data\n",
    "                X_test_trf = rfecv.transform(X_test)\n",
    "\n",
    "                # Run test and get scores, predictions and model\n",
    "                scores, model, y_pred, fit_time = run_test(X_train_trf, X_test_trf, y_train, y_test, model, scorer)\n",
    "\n",
    "                # Write scores, reduced features, and fit time to file\n",
    "                features_threshold_str = str(features_threshold).replace('.', '_')\n",
    "                filename = f\"{experiment_dir_results}/{dataset_name}_{model.__class__.__name__}_ft{features_threshold_str}.txt\"\n",
    "\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(f\"Scores: {scores}\\n\")\n",
    "                    f.write(f\"Fit Time: {fit_time}\\n\")\n",
    "                    f.write(f\"Reduced Features: {reduced_features}\\n\")\n",
    "                # Save best model if specified\n",
    "                if save_best_model:\n",
    "                    save_model(model, f\"{experiment_dir_best_models}/{dataset_name}_{model.__class__.__name__}_ft{features_threshold_str}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(**experiments[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_dataset('prior_cumu_stats_net')\n",
    "y = get_dataset('facts_boxscores_win_result').values.flatten()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)\n",
    "# scores, model, y_pred, fit_time  = run_test(X_train, X_test, y_train, y_test,LogisticRegression(),['accuracy_score',f1_score])\n",
    "# scores\n",
    "# model = LogisticRegression()\n",
    "# rfecv = SelectPercentile(f_classif,percentile=30)\n",
    "# rfecv.fit(X_train, y_train)\n",
    "# reduced_features = list(X.columns[rfecv.get_support()])                 # Get list of reduced features\n",
    "# X_train_trf = rfecv.transform(X_train)                 # Get transformed training and testing data\n",
    "# X_test_trf = rfecv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "\n",
    "def run_experiment(experiment_type,parameters):\n",
    "    return get_experiment(experiment_type)(**parameters)\n",
    "\n",
    "def get_experiment(experiment_type):\n",
    "    EXPERIMENTS = {\n",
    "        'rfecv': rfecv_experiment,\n",
    "        'select_percentile': select_percentile_experiment\n",
    "    }\n",
    "    if experiment_type not in EXPERIMENTS:\n",
    "        raise ValueError(f\"Invalid experiment type '{experiment_type}'\")\n",
    "    return EXPERIMENTS[experiment_type]\n",
    "\n",
    "def select_percentile_experiment(name=None, save_to='.', datasets=None, combine=False, label=None, models=None, scorer=None, save_best_model=False, fs_scorer=None, fs_thresholds=[20,40,60,80,100]):\n",
    "    # Set default name if not provided\n",
    "    if name is None:\n",
    "        name = generate_name()\n",
    "    print(name)\n",
    "    # Create experiment directory   \n",
    "    experiment_dir = f\"{save_to}/{name}\"\n",
    "    experiment_dir_results = f\"{experiment_dir}/results\"\n",
    "    experiment_dir_best_models = f\"{experiment_dir}/best_models\"\n",
    "    experiment_dir_checkpoint = f\"{experiment_dir}/checkpoint.pkl\"\n",
    "    if Path(experiment_dir_checkpoint).exists():\n",
    "        with open(experiment_dir_checkpoint, 'rb') as f:\n",
    "            i_checkpoint, j_checkpoint, m_checkpoint, k_checkpoint = pickle.load(f)\n",
    "        print(f'Continue from checkpoint = {(i_checkpoint, j_checkpoint,m_checkpoint, k_checkpoint)}')\n",
    "    else:\n",
    "        Path(experiment_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(experiment_dir_results).mkdir(parents=True, exist_ok=True)\n",
    "        Path(experiment_dir_best_models).mkdir(parents=True, exist_ok=True)\n",
    "        save_experiment(\n",
    "            dict(name=name, save_to=experiment_dir, datasets=datasets, combine=combine, label=label, \n",
    "                    models=models, scorer=scorer, save_best_model=save_best_model, fs_thresholds=fs_thresholds),\n",
    "            filepath= f'{experiment_dir}/experiment.yaml'\n",
    "        )\n",
    "        i_checkpoint, j_checkpoint, m_checkpoint, k_checkpoint = 0,0,0,0\n",
    "\n",
    "    dataset_iterator = get_datasets(datasets, combine)     # Get dataset iterator\n",
    "    y = get_dataset(label).values.flatten()     # Get label array\n",
    "    model_list = get_models(models)     # Get list of models\n",
    "    features_thresholds = np.unique(to_list(fs_thresholds)).astype(int) \n",
    "    i_len = None\n",
    "    j_len = len(to_list(features_thresholds))\n",
    "    m_len = len(to_list(fs_scorer))\n",
    "    k_len = len(to_list(model_list))\n",
    "    resume_pos = i_checkpoint*j_len+j_checkpoint*m_len + m_checkpoint*k_len+k_checkpoint\n",
    "    dataset_iterator = tqdm(dataset_iterator,initial = resume_pos)\n",
    "\n",
    "    for i,(dataset_name, dataset) in enumerate(dataset_iterator):\n",
    "        if i < i_checkpoint:\n",
    "            continue\n",
    "        # Split data into train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.3, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        X_train.loc[:] = scaler.fit_transform(X_train)\n",
    "        X_test.loc[:] = scaler.fit_transform(X_test)\n",
    "\n",
    "        features_bitmaps_seen = []\n",
    "        for j,pctl_threshold in enumerate(features_thresholds):\n",
    "            if i == i_checkpoint  and j < j_checkpoint:\n",
    "                continue\n",
    "            \n",
    "            fs_scorers = [get_attr_from_module(fs_scorer_str,'sklearn.feature_selection') for fs_scorer_str in to_list(fs_scorer)]\n",
    "            for m,fs_scorer_func in enumerate(fs_scorers):\n",
    "                if i == i_checkpoint and j == j_checkpoint and m < m_checkpoint:\n",
    "                    continue\n",
    "            \n",
    "                fea_selection = SelectPercentile(fs_scorer_func,percentile=pctl_threshold)\n",
    "                fea_selection.fit(X_train, y_train)\n",
    "                features_bitmaps = fea_selection.get_support()\n",
    "                if tuple(features_bitmaps) in features_bitmaps_seen:\n",
    "                    dataset_iterator.update(k_len)\n",
    "                    pass\n",
    "                else:\n",
    "                    features_bitmaps_seen.append(tuple(features_bitmaps))\n",
    "                    reduced_features = list(dataset.columns[features_bitmaps])                 # Get list of reduced features\n",
    "                    X_train_trf = X_train[reduced_features]\n",
    "                    X_test_trf = X_test[reduced_features]\n",
    "\n",
    "                    for k,model in enumerate(model_list):\n",
    "                        # Loop over min features thresholds\n",
    "                        if i == i_checkpoint and j == j_checkpoint and m == m_checkpoint and k < k_checkpoint:\n",
    "                            continue\n",
    "\n",
    "                        dataset_iterator.set_description(\n",
    "                            f'{dataset_name=} ({i+1})' +\n",
    "                            f'model={model.__class__.__name__} ({k+1}/{len(model_list)}),' +\n",
    "                            f'fs_scorer_func={fs_scorer_func.__name__} ({m+1}/{len(fs_scorer)})'\n",
    "                            f'num_features={pctl_threshold}/{len(dataset.columns)} ({j+1}/{len(features_thresholds)})'\n",
    "                        )\n",
    "\n",
    "                        # Run test and get scores, predictions and model\n",
    "                        scores, model, y_pred, fit_time = run_test(X_train_trf, X_test_trf, y_train, y_test, model, scorer)\n",
    "                        # Write scores, reduced features, and fit time to file\n",
    "                        features_threshold_str = str(pctl_threshold).replace('.', '_')\n",
    "                        score_name,val = list(scores.items())[0]\n",
    "                        score_str = '_'.join([score_name[:3],str(val).replace('.', '_')])\n",
    "\n",
    "\n",
    "                        exp_str = dataset_iterator.n\n",
    "                        if exp_str < 10: exp_str = f'00{exp_str}'\n",
    "                        elif exp_str < 100: exp_str = f'0{exp_str}'\n",
    "                        else: exp_str = f'{exp_str}'\n",
    "\n",
    "                        filename = f\"{experiment_dir_results}/{exp_str}_{score_str}_{dataset_name}_{model.__class__.__name__}_ft{len(reduced_features)}_{len(dataset.columns)}.txt\"\n",
    "\n",
    "                        with open(filename, 'w') as f:\n",
    "                            f.write(f\"Scores: {scores}\\n\")\n",
    "                            f.write(f\"Fit Time: {fit_time}\\n\")\n",
    "                            f.write(f\"Num Features: {len(reduced_features)}/{len(dataset.columns)}\\n\")\n",
    "                            f.write(f\"Features: {reduced_features}\\n\")\n",
    "                        # Save best model if specified\n",
    "                        if save_best_model:\n",
    "                            save_model(model, f\"{experiment_dir_best_models}/{dataset_name}_{model.__class__.__name__}_ft{len(reduced_features)}.pkl\")\n",
    "                        \n",
    "                        dataset_iterator.update()\n",
    "\n",
    "                    del X_train_trf, X_test_trf, scores, y_pred, fit_time\n",
    "                    gc.collect()\n",
    "                    \n",
    "                with open(experiment_dir_checkpoint, 'wb') as f:\n",
    "                    pickle.dump((i, j, m, k), f)\n",
    "\n",
    "\n",
    "\n",
    "    if Path(experiment_dir_checkpoint).exists():\n",
    "        os.remove(experiment_dir_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_type': 'select_percentile',\n",
       " 'parameters': {'name': None,\n",
       "  'save_to': './experiments',\n",
       "  'save_best_model': False,\n",
       "  'datasets': ['prior_last10_stats_net',\n",
       "   'prior_cumu_stats_net',\n",
       "   'prior_cumu_stats_rankings_net',\n",
       "   'prior_cumu_win_records_net',\n",
       "   'prior_scheduling'],\n",
       "  'combine': True,\n",
       "  'label': 'facts_boxscores_net_pts',\n",
       "  'models': ['LinearRegression', 'Ridge'],\n",
       "  'scorer': ['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
       "  'fs_scorer': ['r_regression', 'f_regression', 'mutual_info_regression'],\n",
       "  'fs_thresholds': [20, 40, 60, 80, 100]}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = load_experiment('experiments.yaml')\n",
    "experiments[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230420_115540_tasty_lemon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset_name='prior_last10_stats_net|prior_cumu_stats_net|prior_cumu_stats_rankings_net|prior_cumu_win_records_net|prior_scheduling' (31)model=Ridge (2/2),fs_scorer_func=r_regression (1/3)num_features=100/181 (5/5): : 31it [32:53, 63.65s/it]                     \n"
     ]
    }
   ],
   "source": [
    "run_experiment(**experiments[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import *\n",
    "\n",
    "# X = get_dataset('prior_cumu_win_records_net')\n",
    "# y = get_dataset('facts_boxscores_win_result').values.flatten()\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)\n",
    "# # scores, model, y_pred, fit_time  = run_test(X_train, X_test, y_train, y_test,LogisticRegression(),['accuracy_score',f1_score])\n",
    "# # scores\n",
    "\n",
    "# X_train\n",
    "# scaler = StandardScaler()\n",
    "# X_train.loc[:] = scaler.fit_transform(X_train)\n",
    "# X_test.loc[:] = scaler.fit_transform(X_test)\n",
    "# X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
