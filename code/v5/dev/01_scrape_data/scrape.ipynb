{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bs4\n",
    "# %pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9 in /Users/jasetran/Jase/UM/Git/basketball-analytics/.venv/lib/python3.9/site-packages (from html5lib) (1.16.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, html5lib\n",
      "Successfully installed html5lib-1.1 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import *\n",
    "from bs4.element import PageElement\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "HOME_PAGE = 'https://www.basketball-reference.com'\n",
    "SEASONS_PAGE = 'https://www.basketball-reference.com/leagues'\n",
    "TEAMS_PAGE = 'https://www.basketball-reference.com/teams'\n",
    "BOXSCORES_PAGE = 'https://www.basketball-reference.com/boxscores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url,from_local=False):\n",
    "    if from_local:\n",
    "        try:\n",
    "            data = load(url)\n",
    "            return re.sub(\"<!--|-->\",\"\\n\",data)\n",
    "        except:\n",
    "            print(f'Failed to fetch {url} from local. Try to fetch online')\n",
    "    try:\n",
    "        if not url.startswith(\"https://\"):\n",
    "            url = \"https://\"+url\n",
    "        session = requests.Session()\n",
    "        return re.sub(\"<!--|-->\",\"\\n\",session.get(url).text)\n",
    "    except:\n",
    "        print(f'Failed to fetch {url} from web. Please double check url.')\n",
    "        return None\n",
    "\n",
    "def make_soup(text):\n",
    "    return BeautifulSoup(text,features='html.parser')\n",
    "\n",
    "def save(a,filepath,mode='w',file_type=None):\n",
    "    if not Path(filepath).exists():\n",
    "        Path(filepath).parent.mkdir(parents=True,exist_ok=True)\n",
    "    if file_type is None:\n",
    "        with open(filepath,mode) as f:\n",
    "            f.write(a)\n",
    "    elif file_type.endswith('json'):\n",
    "        with open(filepath,mode) as f:\n",
    "            json.dump(a,f)\n",
    "    elif file_type.endswith('pkl'):\n",
    "        with open(filepath,mode) as f:\n",
    "            pickle.dump(a,f)       \n",
    "\n",
    "def load(filepath,mode='r',file_type=None):\n",
    "    if file_type is None:\n",
    "        with open(filepath,mode) as f:\n",
    "            data = f.read()\n",
    "    elif file_type.endswith('json'):\n",
    "        with open(filepath,mode) as f:\n",
    "            data = json.load(f)\n",
    "    elif file_type.endswith('pkl'):\n",
    "        with open(filepath,mode) as f:\n",
    "            data = pickle.load(f)    \n",
    "    return data\n",
    "\n",
    "HOST = 'https://www.basketball-reference.com'\n",
    "URL = f'{HOST}/leagues'\n",
    "html_text = fetch_html(URL)\n",
    "html_soup = make_soup(html_text)\n",
    "# html_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PAGE = 'https://www.basketball-reference.com'\n",
    "SEASONS_PAGE = 'https://www.basketball-reference.com/leagues'\n",
    "TEAMS_PAGE = 'https://www.basketball-reference.com/teams'\n",
    "BOXSCORES_PAGE = 'https://www.basketball-reference.com/boxscores'\n",
    "\n",
    "def fetch_seasons_hrefs(save_to=None,from_local=False):\n",
    "    # fetch leagues page\n",
    "    html_text = fetch_html(SEASONS_PAGE,from_local)\n",
    "    html_soup = make_soup(html_text)\n",
    "    seasons_list = [a['href'] for th in html_soup.find_all('th', {'data-stat': 'season'}) for a in th.find_all('a')]\n",
    "    if save_to:\n",
    "        save(seasons_list,save_to)\n",
    "    return seasons_list\n",
    "\n",
    "def fetch_season_boxscores_hrefs(season_href,save_to=None,from_local=False,sleep=0):\n",
    "    # Load and save season schedule page\n",
    "    # Check for filters. If so iterate through each filter to get the entire list. Else use the schedule on the current page\n",
    "    url = f\"{HOME_PAGE}{season_href.strip('.html')}_games.html\"\n",
    "    html_text = fetch_html(url,from_local)\n",
    "    if html_text is None:\n",
    "        return\n",
    "    if save_to:\n",
    "        save_url = f'{save_to}{url}'\n",
    "        save(html_text,save_url)\n",
    "        \n",
    "    html_soup = make_soup(html_text)\n",
    "    season_boxscores_hrefs = []\n",
    "    filter_div = html_soup.find('div',{'class':'filter'}) \n",
    "    schedule_table = html_soup.find('table', {'id': 'schedule'})\n",
    "    \n",
    "    if filter_div is None:\n",
    "        season_boxscores_hrefs = [a['href'] for th in schedule_table.find_all('td',{'data-stat':'box_score_text'}) for a in th]\n",
    "    \n",
    "    # If so iterate through each filter to get the entire list\n",
    "    else: \n",
    "        month_hrefs = [a['href'] for a in filter_div.select('a')]\n",
    "        for month_href in month_hrefs:\n",
    "            url = f'{HOME_PAGE}{month_href}'\n",
    "            html_text = fetch_html(url,from_local)\n",
    "            if html_text is None:\n",
    "                continue\n",
    "            if save_to:\n",
    "                save_url = f'{save_to}{url}'\n",
    "                save(html_text,save_url)\n",
    "\n",
    "            html_soup = make_soup(html_text)\n",
    "            schedule_table = html_soup.find('table', {'id': 'schedule'})\n",
    "            season_boxscores_hrefs += [a['href'] for th in schedule_table.find_all('td',{'data-stat':'box_score_text'}) for a in th]\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "    return season_boxscores_hrefs\n",
    "\n",
    "def fetch_match_boxscores(boxscore_href,save_to=None,from_local=None, sleep=0):\n",
    "    url = f\"{HOME_PAGE}{boxscore_href}\"\n",
    "    html_text = fetch_html(url,from_local)\n",
    "\n",
    "    box_scores_hrefs = []\n",
    "    filter_div = html_soup.find('div',{'class':'filter'})\n",
    "    if filter_div is not None:\n",
    "        filter_hrefs = [a['href'] for a in filter_div.select('a')]\n",
    "        for filter_href in filter_hrefs:\n",
    "            url = f'{HOME_PAGE}{filter_href}'\n",
    "            html_text = fetch_html(url,from_local)\n",
    "            if html_text is None:\n",
    "                continue\n",
    "            if save_to:\n",
    "                save_url = f'{save_to}{url}'\n",
    "                save(html_text,save_url)\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "            box_scores_hrefs.append(filter_href)\n",
    "\n",
    "    else:\n",
    "        url = f'{HOME_PAGE}{boxscore_href}'\n",
    "        html_text = fetch_html(url,from_local)\n",
    "        if html_text is None:\n",
    "            pass\n",
    "        else:\n",
    "            if save_to:\n",
    "                save_url = f'{save_to}{url}'\n",
    "                save(html_text,save_url)\n",
    "            box_scores_hrefs.append(boxscore_href)\n",
    "    return box_scores_hrefs\n",
    "\n",
    "\n",
    "# def fetch_player(player):\n",
    "#     # pass\n",
    "#     pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # fetch seasons\n",
    "    save_to = './'\n",
    "    seasons_hrefs = fetch_seasons_hrefs(save_to=save_to)\n",
    "    for seasons_href in tqdm(seasons_hrefs,position=0)[:2]:\n",
    "        # fetch season boxscores list\n",
    "        season_boxscores_hrefs = fetch_season_boxscores_hrefs(seasons_href,save_to=save_to,from_local=True, sleep=3)\n",
    "        for season_boxscores_href in tqdm(season_boxscores_hrefs,position=1,leave=True):\n",
    "            # fetch match box scores\n",
    "            match_boxscores = fetch_match_boxscores(season_boxscores_href,save_to=save_to,from_local=False, sleep=3)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/boxscores/202210180BOS.html',\n",
       " '/boxscores/pbp/202210180BOS.html',\n",
       " '/boxscores/shot-chart/202210180BOS.html',\n",
       " '/boxscores/plus-minus/202210180BOS.html']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_match_boxscores(season_boxscores_hrefs[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
