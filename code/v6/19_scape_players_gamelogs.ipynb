{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_tools import *\n",
    "from request_tools import *\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from parse_tools import *\n",
    "from IPython.display import clear_output,HTML\n",
    "from bs4 import BeautifulSoup, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep = 4\n",
    "TGT_DIR = './01-data-html'\n",
    "\n",
    "def request_html(url,content_only=False):\n",
    "    html_text, html_soup = None, None\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        html_text = r.text\n",
    "        html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        if content_only:\n",
    "            html_soup = html_soup.find('div',{'id':'content'})\n",
    "            html_text = html_soup.prettify()\n",
    "    except:\n",
    "        if int(r.status_code) == 429:\n",
    "            print(f\"Too many requests, sleeping for {r.headers['Retry-After']} seconds, starting at {time.ctime()}\")\n",
    "            time.sleep(int(r.headers['Retry-After']))\n",
    "    return html_text, html_soup\n",
    "\n",
    "def load_html(url,content_only=False):\n",
    "    html_text = load_file(url)\n",
    "    html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    if content_only:\n",
    "        html_soup = html_soup.find('div',{'id':'content'})\n",
    "        html_text = html_soup.prettify()\n",
    "    return html_text, html_soup\n",
    "\n",
    "def fetch_all_players_hrefs() -> list:\n",
    "    players_hrefs = []\n",
    "    # For letters in the alphabet\n",
    "    TQDM_LETTERS = tqdm('abcdefghijklmnopqrstuvwxyz',ncols=150)\n",
    "    for letter in TQDM_LETTERS:\n",
    "        TQDM_LETTERS.set_description(letter)\n",
    "        # Get the html soup for the letter page\n",
    "        html_text, html_soup = request_html('/'.join([PLAYERS_PAGE,letter]))\n",
    "        # For each player in the letter page\n",
    "        if html_soup:\n",
    "            for th in html_soup.find_all('th', {'data-stat': 'player'}):\n",
    "                for a in th.find_all('a'):\n",
    "                    players_hrefs.append(a['href'])\n",
    "            save_file('./00-data-facts/players_hrefs.txt','\\n'.join(players_hrefs))\n",
    "        time.sleep(sleep)\n",
    "    # Save the list of player hrefs to a file\n",
    "    save_file('./00-data-facts/players_hrefs.txt','\\n'.join(players_hrefs))\n",
    "    return players_hrefs\n",
    "\n",
    "\n",
    "def scrape_all_player_htmls(TGT_DIR) -> None:\n",
    "    _FAILS_ = []\n",
    "    # Load the list of player hrefs from a file\n",
    "    players_hrefs = load_file('./00-data-facts/players_hrefs.txt').split('\\n')\n",
    "    # For each player href\n",
    "    TQDM_PLAYER_HREFS = tqdm(players_hrefs,ncols=150)\n",
    "    for player_href in TQDM_PLAYER_HREFS:\n",
    "        # Get the html soup for the player page\n",
    "        TQDM_PLAYER_HREFS.set_description(player_href)\n",
    "        try:\n",
    "            html_text, html_soup = request_html('/'.join([HOME_PAGE,player_href]),content_only=True)\n",
    "            save_file('/'.join([TGT_DIR,player_href]), html_text) \n",
    "            time.sleep(sleep)\n",
    "        except Exception as e:\n",
    "            print(f'Error getting {player_href}: {e}')\n",
    "            _FAILS_.append(player_href)\n",
    "    if _FAILS_:\n",
    "        fails = '\\n'.join(_FAILS_)\n",
    "        print(f\"Failed to fetch {len(_FAILS_)} boxscores: {fails}\")\n",
    "    else:\n",
    "        print('All boxscores fetched')\n",
    "\n",
    "\n",
    "def extract_all_players_gamelog_hrefs(SRC_DIR):\n",
    "    def extract_player_gamelog_hrefs(html_soup):\n",
    "            gamelog_hrefs = []\n",
    "            # For each player in the letter page\n",
    "            for a in html_soup.select('div[id=\"bottom_nav\"] a[href*=\"gamelog\"]'):\n",
    "                if '/gamelog/' in a['href']:\n",
    "                    gamelog_hrefs.append(a['href'])\n",
    "                    gamelog_hrefs.append(re.sub('gamelog','gamelog-advanced',a['href']))\n",
    "            return gamelog_hrefs\n",
    "    \n",
    "    # Load the list of player hrefs from a file\n",
    "    ALL_PLAYERS_HREFS_LIST = load_file('./00-data-facts/players_hrefs.txt').split('\\n')\n",
    "    # For each player href\n",
    "    _FAILS_ = []\n",
    "    all_players_gamelog_hrefs_dict = {}\n",
    "    TQDM_PLAYER_HREFS = tqdm(ALL_PLAYERS_HREFS_LIST,ncols=150)\n",
    "    for player_href in TQDM_PLAYER_HREFS:\n",
    "        # Get the html soup for the player page\n",
    "        TQDM_PLAYER_HREFS.set_description(player_href)\n",
    "        try:\n",
    "            html_text,html_soup = load_html('/'.join([SRC_DIR,player_href]))\n",
    "            gamelog_hrefs       = extract_player_gamelog_hrefs(html_soup)\n",
    "            all_players_gamelog_hrefs_dict[player_href] = gamelog_hrefs\n",
    "            save_json('./00-data-facts/players_gamelog_hrefs.json', all_players_gamelog_hrefs_dict)\n",
    "        except Exception as e:\n",
    "            # print(f'Error getting {player_href}: {e}\\r')\n",
    "            _FAILS_.append(player_href)\n",
    "            continue\n",
    "        # time.sleep(sleep)\n",
    "    if _FAILS_:\n",
    "        fails = '\\n'.join(_FAILS_)\n",
    "        print(f\"Failed to fetch {len(_FAILS_)} cases: {fails}\")\n",
    "    else:\n",
    "        print('All cases extracted')\n",
    "\n",
    "\n",
    "def scrape_all_players_gamelog_html(TGT_DIR):\n",
    "    # Load the list of player gamelog hrefs from a file\n",
    "    SRC_DIR = './00-data-facts/players_gamelog_hrefs.json'\n",
    "    players_gamelog_hrefs_dict = load_json(SRC_DIR)\n",
    "    # For each player href\n",
    "    _FAILS_ = []\n",
    "    TQDM_PLAYER_HREFS = tqdm(players_gamelog_hrefs_dict.items(),ncols=150)\n",
    "    for player_href,player_gamelog_href_list in TQDM_PLAYER_HREFS:\n",
    "        # Get the html soup for the player page\n",
    "        TQDM_PLAYER_HREFS.set_description(f'{player_href} ({len(player_gamelog_href_list)})')\n",
    "        for player_gamelog_href in player_gamelog_href_list:\n",
    "            try:\n",
    "                html_text, html_soup = request_html('/'.join([HOME_PAGE,player_gamelog_href]),content_only=True)\n",
    "                save_file('/'.join([TGT_DIR,player_gamelog_href+'.html']), html_text) \n",
    "            except Exception as e:\n",
    "                print(f'Error getting {player_gamelog_href}: {e}')\n",
    "                _FAILS_.append(player_gamelog_href)\n",
    "            time.sleep(sleep)\n",
    "    if _FAILS_:\n",
    "        fails = '\\n'.join(_FAILS_)\n",
    "        print(f\"Failed to fetch {len(_FAILS_)} cases: {fails}\")\n",
    "    else:\n",
    "        print('All cases fetched')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT_DIR = './01-data-html'\n",
    "\n",
    "# fetch_all_players_hrefs()\n",
    "# scrape_all_player_htmls(TGT_DIR)\n",
    "# extract_all_players_gamelog_hrefs('./01-data-html')\n",
    "scrape_all_players_gamelog_html(TGT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/players/b/battlke01/gamelog-advanced/1992',\n",
       " '/players/b/baumjo01/gamelog-advanced/1972/aba/',\n",
       " '/players/b/baumjo01/gamelog/1973/aba/',\n",
       " '/players/b/bayloel01/gamelog/1969',\n",
       " '/players/f/frahmri01/gamelog/2006',\n",
       " '/players/f/frahmri01/gamelog-advanced/2006',\n",
       " '/players/f/francst01/gamelog/2002',\n",
       " '/players/f/frankte01/gamelog/1990',\n",
       " '/players/f/frankja01/gamelog-advanced/2014',\n",
       " '/players/g/gazean01/gamelog/1994',\n",
       " '/players/l/lewisra02/gamelog/2001',\n",
       " '/players/l/liberma01/gamelog/1992',\n",
       " '/players/l/lichtto01/gamelog/1991',\n",
       " '/players/l/liggide01/gamelog/2014',\n",
       " '/players/l/ligongo01/gamelog-advanced/1968/aba/',\n",
       " '/players/l/ligongo01/gamelog-advanced/1971/aba/',\n",
       " '/players/n/nelsodo01/gamelog-advanced/1972',\n",
       " '/players/r/raycl01/gamelog-advanced/1978',\n",
       " '/players/t/tollian01/gamelog/2009',\n",
       " '/players/t/tomjaru01/gamelog/1981',\n",
       " '/players/t/toneyan01/gamelog/1988',\n",
       " '/players/t/toomaja01/gamelog-advanced/1950',\n",
       " '/players/w/williho01/gamelog-advanced/1996',\n",
       " '/players/y/youngni01/gamelog/2019',\n",
       " '/players/y/youngsa01/gamelog/2010']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAILS = \"\"\"\\\n",
    "/players/b/battlke01/gamelog-advanced/1992\n",
    "/players/b/baumjo01/gamelog-advanced/1972/aba/\n",
    "/players/b/baumjo01/gamelog/1973/aba/\n",
    "/players/b/bayloel01/gamelog/1969\n",
    "/players/f/frahmri01/gamelog/2006\n",
    "/players/f/frahmri01/gamelog-advanced/2006\n",
    "/players/f/francst01/gamelog/2002\n",
    "/players/f/frankte01/gamelog/1990\n",
    "/players/f/frankja01/gamelog-advanced/2014\n",
    "/players/g/gazean01/gamelog/1994\n",
    "/players/l/lewisra02/gamelog/2001\n",
    "/players/l/liberma01/gamelog/1992\n",
    "/players/l/lichtto01/gamelog/1991\n",
    "/players/l/liggide01/gamelog/2014\n",
    "/players/l/ligongo01/gamelog-advanced/1968/aba/\n",
    "/players/l/ligongo01/gamelog-advanced/1971/aba/\n",
    "/players/n/nelsodo01/gamelog-advanced/1972\n",
    "/players/r/raycl01/gamelog-advanced/1978\n",
    "/players/t/tollian01/gamelog/2009\n",
    "/players/t/tomjaru01/gamelog/1981\n",
    "/players/t/toneyan01/gamelog/1988\n",
    "/players/t/toomaja01/gamelog-advanced/1950\n",
    "/players/w/williho01/gamelog-advanced/1996\n",
    "/players/y/youngni01/gamelog/2019\n",
    "/players/y/youngsa01/gamelog/2010\\\n",
    "\"\"\".split('\\n')\n",
    "\n",
    "FAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_gamelog_href = FAILS[2]\n",
    "\n",
    "html_text, html_soup = request_html('/'.join([HOME_PAGE,player_gamelog_href]),content_only=True)\n",
    "# HTML(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/players/y/youngsa01/gamelog/2010: 100%|██████████████████████████████████████████████████████████████████████████████| 25/25 [02:28<00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All cases fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SRC_DIR = './00-data-facts/players_gamelog_hrefs.json'\n",
    "# players_gamelog_hrefs_dict = load_json(SRC_DIR)\n",
    "# For each player href\n",
    "TGT_DIR = './01-data-html'\n",
    "_FAILS_ = []\n",
    "TQDM_PLAYER_HREFS = tqdm(FAILS,ncols=150)\n",
    "    # Get the html soup for the player page\n",
    "for player_gamelog_href in TQDM_PLAYER_HREFS:\n",
    "    TQDM_PLAYER_HREFS.set_description(f'{player_gamelog_href}')\n",
    "    try:\n",
    "        html_text, html_soup = request_html('/'.join([HOME_PAGE,player_gamelog_href]),content_only=True)\n",
    "        save_file('/'.join([TGT_DIR,player_gamelog_href+'.html']), html_text) \n",
    "    except Exception as e:\n",
    "        print(f'Error getting {player_gamelog_href}: {e}')\n",
    "        _FAILS_.append(player_gamelog_href)\n",
    "    time.sleep(sleep)\n",
    "if _FAILS_:\n",
    "    fails = '\\n'.join(_FAILS_)\n",
    "    print(f\"Failed to fetch {len(_FAILS_)} cases: {fails}\")\n",
    "else:\n",
    "    print('All cases fetched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
